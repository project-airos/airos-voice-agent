nodes:
  - id: wserver
    # Note: Pre-installed in Docker image, spawned as static node with node connection
    path: dora-openai-websocket
    args: --node
    inputs:
      audio: primespeech/audio
      asr_transcription: asr/transcription
      asr_log: asr/log
      speech_log: speech-monitor/log
      speech_started: speech-monitor/speech_started
      speech_ended: speech-monitor/speech_ended
      question_ended: speech-monitor/question_ended
      segment_complete: primespeech/segment_complete
      tts_log: primespeech/log
    outputs:
      - audio
      - text
    env:
      HOST: "0.0.0.0"
      PORT: "8123"

  # # Audio player
  # - id: audio-player
  #   path: dynamic
  #   inputs:
  #     audio: primespeech/audio
  #   outputs:
  #     - buffer_status
  #     - status

  # - id: mac-aec
  #   path: dynamic
  #   outputs:
  #     - audio
  #     - is_speaking
  #     - speech_started
  #     - speech_ended
  #     - audio_segment     

  # Speech detection and segmentation with pause/resume capability
  - id: speech-monitor
    # Note: Pre-installed in Docker image, no build needed
    path: dora-speechmonitor
    inputs:
      audio:
        source: wserver/audio
        queue_size: 1000000
      # control: chat-controller/speech_control  # Commented out - no chat-controller in this config
    outputs:
      - speech_started
      - speech_ended
      - question_ended
      - is_speaking
      - audio_segment
      - speech_probability
      - log
    env:
      MIN_AUDIO_AMPLITUDE: 0.005
      ACTIVE_FRAME_THRESHOLD_MS: 60
      USER_SILENCE_THRESHOLD_MS: 1200  # Faster turn end
      SILENCE_THRESHOLD_MS: 400
      QUESTION_END_SILENCE_MS: 1500
      AUDIO_FRAMES_THRESHOLD_MS: 10000
      VAD_THRESHOLD: 0.5
      VAD_ENABLED: true
      SAMPLE_RATE: 16000
      LOG_LEVEL: DEBUG

  
  # ASR transcription
  - id: asr
    # Note: Pre-installed in Docker image, no build needed
    path: dora-asr
    inputs:
      audio:
        source: speech-monitor/audio_segment
        queue_size: 10
    outputs:
      - transcription
      - language_detected
      - processing_time
      - confidence
      - log
    env:
      ASR_ENGINE: funasr
      LANGUAGE: zh
      WHISPER_MODEL: large
      ENABLE_PUNCTUATION: true
      ENABLE_LANGUAGE_DETECTION: true
      ENABLE_CONFIDENCE_SCORE: false
      MIN_AUDIO_DURATION: "0.2"
      ASR_MODELS_DIR: ~/.dora/models/asr # Use home-relative path (expanded by process)
      # ASR_MODELS_DIR: ~/.cache/modelscope/hub/models/iic  # Modelscope cache location
      # If not set, defaults to ~/.dora/models/asr (which has the models)
      LOG_LEVEL: DEBUG


  # MaaS Client with Playwright Browser Tools (now static to allow reuse)
  - id: maas-client
    # Run as a static node using the prebuilt binary included in the image
    path: dora-maas-client
    inputs:
      text: asr/transcription  # Direct from ASR
      text_to_audio: wserver/text  # Also receive text from WebSocket for greetings
    outputs:
      - text
      - status
      - log
    env:
      # Use the same config file mounted by docker compose
      MAAS_CONFIG_PATH: /opt/airos/examples/openai-realtime/maas_config.toml
      LOG_LEVEL: INFO

  # Text Segmenter - buffers LLM output and sends to TTS one segment at a time
  - id: text-segmenter
    # Note: Pre-installed in Docker image, no build needed
    path: dora-text-segmenter
    inputs:
      text: maas-client/text  # From MaaS
      tts_complete: primespeech/segment_complete  # TTS completion signal
    outputs:
      - text_segment
      - status
      - metrics
    env:
      ENABLE_BACKPRESSURE: "false"  # Don't wait initially - send first segment immediately
      SEGMENT_MODE: "sentence"  # sentence, punctuation, or fixed
      MIN_SEGMENT_LENGTH: "5"
      MAX_SEGMENT_LENGTH: "20"
      PUNCTUATION_MARKS: "。！？.!?"
      LOG_LEVEL: "INFO"

  # PrimeSpeech TTS
  - id: primespeech
    # Note: Pre-installed in Docker image, no build needed
    path: dora-primespeech
    inputs:
      text: text-segmenter/text_segment  # From text segmenter (not directly from LLM)
    outputs:
      - audio
      - status
      - segment_complete

    env:
      # Model path configuration (REQUIRED)
      # Use absolute path to avoid '~' not expanding inside node environment
      PRIMESPEECH_MODEL_DIR: /root/.dora/models/primespeech  # Path to your models directory
      # Note: Models should be in PRIMESPEECH_MODEL_DIR/moyoyo/ with this structure:
      #   - GPT_weights/doubao_best_gpt.ckpt
      #   - SoVITS_weights/doubao_best_sovits.pth
      #   - ref_audios/doubao_ref.wav
      #   - chinese-hubert-base/
      #   - chinese-roberta-wwm-ext-large/
      
      # Voice selection
      # Available voices: Doubao, Luo Xiang, Yang Mi, Zhou Jielun, Ma Yun, 
      # Maple, Cove, BYS, Ellen, Juniper, Ma Baoguo, Shen Yi, Trump
      VOICE_NAME: Doubao
      
      # Language settings
      TEXT_LANG: zh    # Force Chinese processing only
      PROMPT_LANG: zh  # Language of the reference prompt
      
      # Inference parameters
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.0  # Speech speed multiplier
      
      # Performance
      USE_GPU: false
      NUM_THREADS: 4
      
      RETURN_FRAGMENT: "false"  # Disable streaming TTS for now
      LOG_LEVEL: "DEBUG"
      # Internal text segmentation for faster TTS
      ENABLE_INTERNAL_SEGMENTATION: "true"  # Split long text internally
      TTS_MAX_SEGMENT_LENGTH: "100"  # Max chars per TTS segment
      TTS_MIN_SEGMENT_LENGTH: "20"   # Min chars per TTS segment
      
      # Logging
      LOG_LEVEL: INFO  # DEBUG, INFO, WARNING, ERROR

  # Simple viewer to see the flow
  - id: viewer
    path: dynamic
    inputs:
      transcription: asr/transcription
      llm_output: maas-client/text
      segment: text-segmenter/text_segment
      speech_started: speech-monitor/speech_started
      speech_ended: speech-monitor/speech_ended
